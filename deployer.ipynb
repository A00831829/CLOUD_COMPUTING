{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versiones de paquetes:\n",
      "azureml-core: 1.48.0\n",
      "Python: 3.10.16 (main, Mar 17 2025, 20:54:03) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from azureml.core import Workspace, Model\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# Comprobar las versiones de los paquetes\n",
    "import pkg_resources\n",
    "print(\"Versiones de paquetes:\")\n",
    "print(f\"azureml-core: {pkg_resources.get_distribution('azureml-core').version}\")\n",
    "print(f\"Python: {os.sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando ID de suscripción de Azure...\n",
      "ID de suscripción cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando ID de suscripción de Azure...\")\n",
    "try:\n",
    "    with open('id_CONFIDENTIAL.json', 'r') as id_file:\n",
    "        mi = json.load(id_file)\n",
    "        my_id = mi[\"my_id\"]\n",
    "    print(\"ID de suscripción cargado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el ID de suscripción: {e}\")\n",
    "    print(\"Asegúrate de crear un archivo 'id_CONFIDENTIAL.json' con tu ID de suscripción de Azure.\")\n",
    "    print('Formato: {\"my_id\": \"tu-id-de-suscripción-aquí\"}')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando workspace en Azure en centralindia...\n",
      "Deploying KeyVault with name bankruptkeyvault1ecabe6e.\n",
      "Deploying StorageAccount with name bankruptstorageeeaae05b6.\n",
      "Deploying AppInsights with name bankruptinsightscaefcc4e.\n",
      "Deployed AppInsights with name bankruptinsightscaefcc4e. Took 23.39 seconds.\n",
      "Deployed KeyVault with name bankruptkeyvault1ecabe6e. Took 26.52 seconds.\n",
      "Deploying Workspace with name bankruptcy_workspace_2nd.\n",
      "Deployed StorageAccount with name bankruptstorageeeaae05b6. Took 31.82 seconds.\n",
      "Deployed Workspace with name bankruptcy_workspace_2nd. Took 42.2 seconds.\n",
      "Workspace creado/cargado exitosamente.\n",
      "Detalles del workspace:\n",
      "Nombre: bankruptcy_workspace_2nd\n",
      "Región: centralindia\n",
      "Error al crear el workspace: 'Workspace' object has no attribute 'id'\n"
     ]
    }
   ],
   "source": [
    "# Nombre del grupo de recursos y ubicación\n",
    "resource_group = \"bankruptcy_predictor_rg_diego\"  # Nombre actualizado\n",
    "location = \"centralindia\"  # Puedes cambiar esto según tu preferencia\n",
    "\n",
    "print(f\"Creando workspace en Azure en {location}...\")\n",
    "# Crear workspace de Azure ML\n",
    "try:\n",
    "    ws = Workspace.create(\n",
    "        name=\"bankruptcy_workspace_2nd\",  # Nombre actualizado\n",
    "        subscription_id=my_id,\n",
    "        resource_group=resource_group,\n",
    "        location=location,\n",
    "        exist_ok=True  # No falla si ya existe\n",
    "    )\n",
    "    print(\"Workspace creado/cargado exitosamente.\")\n",
    "    \n",
    "    # Verificar los detalles del workspace\n",
    "    print(\"Detalles del workspace:\")\n",
    "    print(f\"Nombre: {ws.name}\")\n",
    "    print(f\"Región: {ws.location}\")\n",
    "    print(f\"ID de recurso: {ws.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear el workspace: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registrando modelo en Azure ML...\n",
      "Registering model bankruptcy_model_2nd\n",
      "Modelo registrado con ID: bankruptcy_model_2nd:1\n",
      "Nombre del modelo: bankruptcy_model_2nd\n",
      "Versión del modelo: 1\n"
     ]
    }
   ],
   "source": [
    "# Verificar que el modelo exista\n",
    "if not os.path.exists(\"model.pkl\"):\n",
    "    print(\"Error: No se encontró el archivo 'model.pkl'.\")\n",
    "    print(\"Ejecuta primero el script 'modelo.py' para entrenar y guardar el modelo.\")\n",
    "    exit(1)\n",
    "\n",
    "# Registrar el modelo en Azure ML\n",
    "print(\"Registrando modelo en Azure ML...\")\n",
    "model_name = \"bankruptcy_model_2nd\"\n",
    "try:\n",
    "    registered_model = Model.register(\n",
    "        model_path=\"model.pkl\",\n",
    "        model_name=model_name,\n",
    "        workspace=ws\n",
    "    )\n",
    "    print(f\"Modelo registrado con ID: {registered_model.id}\")\n",
    "    print(f\"Nombre del modelo: {registered_model.name}\")\n",
    "    print(f\"Versión del modelo: {registered_model.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al registrar el modelo: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versiones de paquetes:\n",
      "azureml-core: 1.48.0\n",
      "Python: 3.10.16 (main, Mar 17 2025, 20:54:03) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from azureml.core import Workspace, Model\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# Comprobar las versiones de los paquetes\n",
    "import pkg_resources\n",
    "print(\"Versiones de paquetes:\")\n",
    "print(f\"azureml-core: {pkg_resources.get_distribution('azureml-core').version}\")\n",
    "print(f\"Python: {os.sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando umbral de clasificación...\n",
      "Umbral cargado: 0.5500000000000002\n",
      "Creando script de inferencia (score.py)...\n",
      "Script de inferencia creado: score.py\n"
     ]
    }
   ],
   "source": [
    "# Cargar el umbral calculado\n",
    "if not os.path.exists(\"umbral.json\"):\n",
    "    print(\"Error: No se encontró el archivo 'umbral.json'.\")\n",
    "    print(\"Ejecuta primero el script 'modelo.py' para calcular y guardar el umbral.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Cargando umbral de clasificación...\")\n",
    "with open(\"umbral.json\", \"r\") as umb_file:\n",
    "    umb = json.load(umb_file)\n",
    "    umbral = umb[\"umbral\"][0]\n",
    "    print(f\"Umbral cargado: {umbral}\")\n",
    "\n",
    "# Crear el script score.py para inferencia\n",
    "print(\"Creando script de inferencia (score.py)...\")\n",
    "\n",
    "# Contenido del script score.py - VERSIÓN CORREGIDA CON MEJOR MANEJO DE ERRORES\n",
    "scorepy = f\"\"\"\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    try:\n",
    "        # Imprimimos información para depuración\n",
    "        print(\"Python version:\", os.sys.version)\n",
    "        print(\"Current working directory:\", os.getcwd())\n",
    "        \n",
    "        # Obtener la ruta del modelo\n",
    "        model_path = Model.get_model_path('{model_name}')\n",
    "        print(f\"Ruta del modelo: {{model_path}}\")\n",
    "        \n",
    "        # Verificar si el archivo existe\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"ERROR: El archivo del modelo no existe en {{model_path}}\")\n",
    "            raise FileNotFoundError(f\"No se encuentra el modelo en {{model_path}}\")\n",
    "            \n",
    "        # Cargar el modelo\n",
    "        print(\"Cargando el modelo...\")\n",
    "        model = joblib.load(model_path)\n",
    "        print(f\"Modelo cargado exitosamente. Tipo: {{type(model)}}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR en init(): {{str(e)}}\")\n",
    "        print(\"Traceback completo:\")\n",
    "        traceback.print_exc()\n",
    "        # Re-lanzar la excepción para que Azure ML pueda registrarla\n",
    "        raise\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Parsear los datos JSON recibidos\n",
    "        print(\"Recibiendo datos...\")\n",
    "        data = json.loads(raw_data)['data'][0]\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        print(\"Convirtiendo a DataFrame...\")\n",
    "        data = pd.DataFrame(data)\n",
    "        print(f\"Datos recibidos con forma: {{data.shape}}\")\n",
    "        \n",
    "        # Realizar la predicción\n",
    "        print(\"Realizando predicción...\")\n",
    "        result_proba = model.predict_proba(data)[:, 1].tolist()  # Probabilidad de clase positiva\n",
    "        umbral = {umbral}\n",
    "        result_finals = [1 if x > umbral else 0 for x in result_proba]\n",
    "        \n",
    "        print(f\"Predicción completada. Resultados: {{result_finals}}\")\n",
    "        return json.dumps(result_finals)\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"ERROR en run(): {{error_message}}\")\n",
    "        print(\"Traceback completo:\")\n",
    "        traceback.print_exc()\n",
    "        return json.dumps({{\"error\": error_message, \"traceback\": traceback.format_exc()}})\n",
    "\"\"\"\n",
    "\n",
    "# Escribir el archivo score.py\n",
    "with open(\"score.py\", \"w\") as file_score:\n",
    "    file_score.write(scorepy)\n",
    "    print(\"Script de inferencia creado: score.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando entorno de inferencia...\n",
      "Configuración del entorno:\n",
      "# Conda environment specification. The dependencies defined in this file will\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.8 and later.\n",
      "- python=3.8\n",
      "\n",
      "- pip:\n",
      "  - azureml-defaults~=1.48.0\n",
      "  - joblib==1.0.1\n",
      "- numpy=1.19.5\n",
      "- pandas=1.2.4\n",
      "- scikit-learn=0.24.2\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Configurando entorno de inferencia...\")\n",
    "\n",
    "virtual_env = Environment(\"bankruptcy_env_diego\") \n",
    "conda_deps = CondaDependencies.create(\n",
    "    python_version=\"3.10\",  \n",
    "    conda_packages=[\n",
    "        'numpy=1.19.5',\n",
    "        'pandas=1.2.4', \n",
    "        'scikit-learn=0.24.2' \n",
    "    ],\n",
    "    pip_packages=[\n",
    "        'azureml-defaults==1.41.0',\n",
    "        'joblib==1.0.1'\n",
    "    ]\n",
    ")\n",
    "\n",
    "virtual_env.python.conda_dependencies = conda_deps\n",
    "\n",
    "# Configuración de inferencia\n",
    "inference_config = InferenceConfig(\n",
    "    environment=virtual_env,\n",
    "    entry_script=\"score.py\",\n",
    "    source_directory=None\n",
    ")\n",
    "\n",
    "# Mostrar la configuración del entorno para verificar\n",
    "print(\"Configuración del entorno:\")\n",
    "print(virtual_env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desplegando modelo como servicio web...\n",
      "Este proceso puede tardar varios minutos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_15264\\2411632133.py:14: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration. \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  service = Model.deploy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperando a que el servicio se despliegue...\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2025-04-03 19:31:09-06:00 Creating Container Registry if not exists..\n",
      "2025-04-03 19:41:09-06:00 Registering the environment..\n",
      "2025-04-03 19:41:11-06:00 Building image..\n",
      "2025-04-03 19:53:10-06:00 Generating deployment configuration.\n",
      "2025-04-03 19:53:12-06:00 Submitting deployment to compute..\n",
      "2025-04-03 19:53:22-06:00 Checking the status of deployment bankruptcy-prediction-diego..\n",
      "2025-04-03 19:54:41-06:00 Checking the status of inference endpoint bankruptcy-prediction-diego.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "¡Servicio desplegado exitosamente!\n",
      "URI del servicio: http://63282996-37d4-43dc-bf14-fce6f11b4078.centralindia.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "# Configuración del servicio web con más recursos\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,     \n",
    "    memory_gb=2,      \n",
    "    description=\"API para predecir bancarrota empresarial\",\n",
    "    auth_enabled=True  \n",
    ")\n",
    "\n",
    "print(\"Desplegando modelo como servicio web...\")\n",
    "print(\"Este proceso puede tardar varios minutos...\")\n",
    "\n",
    "# Desplegar el modelo\n",
    "try:\n",
    "    service = Model.deploy(\n",
    "        workspace=ws,\n",
    "        name='bankruptcy-prediction-diego', \n",
    "        models=[registered_model],\n",
    "        inference_config=inference_config,\n",
    "        deployment_config=aci_config,\n",
    "        overwrite=True,\n",
    "    )\n",
    "    \n",
    "    # Esperar a que el servicio se despliegue\n",
    "    print(\"Esperando a que el servicio se despliegue...\")\n",
    "    service.wait_for_deployment(show_output=True)\n",
    "    \n",
    "    # Obtener la URI del servicio\n",
    "    scoring_uri = service.scoring_uri\n",
    "    print(f\"¡Servicio desplegado exitosamente!\")\n",
    "    print(f\"URI del servicio: {scoring_uri}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al desplegar el servicio: {e}\")\n",
    "    print(\"Revisa los registros de Azure para más detalles.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs del servicio desplegado:\n",
      "/bin/bash: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2025-04-04T01:54:34,955059865+00:00 - iot-server/run \n",
      "2025-04-04T01:54:34,956937858+00:00 - rsyslog/run \n",
      "bash: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "2025-04-04T01:54:34,975556277+00:00 - nginx/run \n",
      "2025-04-04T01:54:34,986656161+00:00 - gunicorn/run \n",
      "2025-04-04T01:54:34,990140961+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:34,993509163+00:00 | gunicorn/run | ###############################################\n",
      "2025-04-04T01:54:34,997059996+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2025-04-04T01:54:35,000906992+00:00 | gunicorn/run | ###############################################\n",
      "2025-04-04T01:54:35,005829704+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:35,010565040+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:35,016817402+00:00 | gunicorn/run | AzureML image information: openmpi4.1.0-ubuntu20.04, Materializaton Build:20221101.v11\n",
      "2025-04-04T01:54:35,020095046+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:35,022384817+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:35,025036753+00:00 | gunicorn/run | PATH environment variable: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2025-04-04T01:54:35,027293352+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2025-04-04T01:54:35,028319917+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:35,030201509+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "/bin/bash: /azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2025-04-04T01:54:35,260719413+00:00 - iot-server/finish 1 0\n",
      "2025-04-04T01:54:35,263419549+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "adal==1.2.7\n",
      "argcomplete==2.1.2\n",
      "attrs==25.3.0\n",
      "azure-common==1.1.28\n",
      "azure-core==1.33.0\n",
      "azure-graphrbac==0.61.2\n",
      "azure-identity==1.17.1\n",
      "azure-mgmt-authorization==3.0.0\n",
      "azure-mgmt-containerregistry==10.3.0\n",
      "azure-mgmt-core==1.5.0\n",
      "azure-mgmt-keyvault==10.3.1\n",
      "azure-mgmt-resource==21.2.1\n",
      "azure-mgmt-storage==20.1.0\n",
      "azureml-core==1.48.0\n",
      "azureml-dataprep==4.8.6\n",
      "azureml-dataprep-native==38.0.0\n",
      "azureml-dataprep-rslex==2.15.2\n",
      "azureml-dataset-runtime==1.48.0\n",
      "azureml-defaults==1.48.0\n",
      "azureml-inference-server-http==0.7.7\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==4.3.0\n",
      "blinker==1.8.2\n",
      "Brotli @ file:///croot/brotli-split_1714483155106/work\n",
      "cachetools==5.5.2\n",
      "certifi @ file:///croot/certifi_1725551672989/work/certifi\n",
      "cffi==1.17.1\n",
      "charset-normalizer @ file:///croot/charset-normalizer_1721748349566/work\n",
      "click==8.1.8\n",
      "cloudpickle==2.2.1\n",
      "contextlib2==21.6.0\n",
      "cryptography==38.0.4\n",
      "distro==1.9.0\n",
      "docker==6.1.3\n",
      "dotnetcore2==3.1.23\n",
      "Flask==3.0.3\n",
      "Flask-Cors==3.0.10\n",
      "fusepy==3.0.1\n",
      "google-api-core==2.24.2\n",
      "google-auth==2.38.0\n",
      "googleapis-common-protos==1.69.2\n",
      "gunicorn==20.1.0\n",
      "humanfriendly==10.0\n",
      "idna @ file:///croot/idna_1714398848350/work\n",
      "importlib_metadata==8.5.0\n",
      "importlib_resources==6.4.5\n",
      "inference-schema==1.5.1\n",
      "isodate==0.7.2\n",
      "itsdangerous==2.2.0\n",
      "jeepney==0.9.0\n",
      "Jinja2==3.1.6\n",
      "jmespath==1.0.1\n",
      "joblib==1.0.1\n",
      "jsonpickle==2.2.0\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "knack==0.10.1\n",
      "MarkupSafe==2.1.5\n",
      "mkl-fft @ file:///croot/mkl_fft_1695058164594/work\n",
      "mkl-random @ file:///croot/mkl_random_1695059800811/work\n",
      "mkl-service==2.4.0\n",
      "msal==1.32.0\n",
      "msal-extensions==1.0.0\n",
      "msrest==0.7.1\n",
      "msrestazure==0.6.4\n",
      "ndg-httpsclient==0.5.1\n",
      "numpy @ file:///work/mkl/numpy_and_numpy_base_1682950891414/work\n",
      "oauthlib==3.2.2\n",
      "opencensus==0.11.4\n",
      "opencensus-context==0.1.3\n",
      "opencensus-ext-azure==1.1.14\n",
      "packaging==21.3\n",
      "pandas==1.2.4\n",
      "paramiko==2.12.0\n",
      "pathspec==0.12.1\n",
      "pkginfo==1.12.1.2\n",
      "pkgutil_resolve_name==1.3.10\n",
      "platformdirs @ file:///croot/platformdirs_1692205439124/work\n",
      "pooch @ file:///croot/pooch_1695850093751/work\n",
      "portalocker==2.10.1\n",
      "proto-plus==1.26.1\n",
      "protobuf==5.29.4\n",
      "psutil==7.0.0\n",
      "pyarrow==9.0.0\n",
      "pyasn1==0.6.1\n",
      "pyasn1_modules==0.4.2\n",
      "pycparser==2.22\n",
      "Pygments==2.19.1\n",
      "PyJWT==2.9.0\n",
      "PyNaCl==1.5.0\n",
      "pyOpenSSL==22.1.0\n",
      "pyparsing==3.1.4\n",
      "PySocks @ file:///tmp/build/80754af9/pysocks_1605305779399/work\n",
      "python-dateutil @ file:///croot/python-dateutil_1716495738603/work\n",
      "pytz @ file:///croot/pytz_1713974312559/work\n",
      "PyYAML==6.0.2\n",
      "referencing==0.35.1\n",
      "requests @ file:///croot/requests_1721410876868/work\n",
      "requests-oauthlib==2.0.0\n",
      "rpds-py==0.20.1\n",
      "rsa==4.9\n",
      "scikit-learn @ file:///tmp/build/80754af9/scikit-learn_1621370412049/work\n",
      "scipy==1.10.1\n",
      "SecretStorage==3.3.3\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "tabulate==0.9.0\n",
      "threadpoolctl @ file:///croot/threadpoolctl_1719407800858/work\n",
      "typing_extensions==4.13.1\n",
      "urllib3==1.26.20\n",
      "websocket-client==1.8.0\n",
      "Werkzeug==3.0.6\n",
      "wrapt==1.12.1\n",
      "zipp==3.20.2\n",
      "\n",
      "2025-04-04T01:54:35,813985463+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:35,815177216+00:00 | gunicorn/run | ###############################################\n",
      "2025-04-04T01:54:35,816296964+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2025-04-04T01:54:35,817881350+00:00 | gunicorn/run | ###############################################\n",
      "2025-04-04T01:54:35,821450888+00:00 | gunicorn/run | \n",
      "2025-04-04T01:54:36,409885544+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.7.7\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/score.py\n",
      "Model Directory: /var/azureml-app/azureml-models/bankruptcy_model_2nd/1\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.7.7\n",
      "CORS for the specified origins: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://0.0.0.0:31311 (72)\n",
      "Using worker: sync\n",
      "Booting worker with pid: 126\n",
      "Initializing logger\n",
      "2025-04-04 01:54:37,366 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2025-04-04 01:54:37,366 | root | INFO | Starting up app insight hooks\n",
      "2025-04-04 01:54:39,164 | root | INFO | Found user script at /var/azureml-app/score.py\n",
      "2025-04-04 01:54:39,164 | root | INFO | run() is not decorated. Server will invoke it with the input in JSON string.\n",
      "2025-04-04 01:54:39,164 | root | INFO | Invoking user's init function\n",
      "00000000-0000-0000-0000-000000000000,Python version:\n",
      "00000000-0000-0000-0000-000000000000,/azureml-envs/azureml_605d10292a84721aabef4fa7b6aba91d/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "00000000-0000-0000-0000-000000000000,3.8.20 (default, Oct  3 2024, 15:24:27) \n",
      "[GCC 11.2.0]\n",
      "00000000-0000-0000-0000-000000000000,Current working directory:\n",
      "00000000-0000-0000-0000-000000000000,/var/azureml-app\n",
      "00000000-0000-0000-0000-000000000000,Ruta del modelo: /var/azureml-app/azureml-models/bankruptcy_model_2nd/1/model.pkl\n",
      "00000000-0000-0000-0000-000000000000,Cargando el modelo...\n",
      "00000000-0000-0000-0000-000000000000,Modelo cargado exitosamente. Tipo: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "2025-04-04 01:54:39,842 | root | INFO | Users's init has completed successfully\n",
      "2025-04-04 01:54:39,844 | root | INFO | Swaggers are prepared for the following versions: [2, 3].\n",
      "2025-04-04 01:54:39,844 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2025-04-04 01:54:39,845 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "2025-04-04 01:54:41,940 | root | INFO | 200\n",
      "127.0.0.1 - - [04/Apr/2025:01:54:41 +0000] \"GET /swagger.json HTTP/1.0\" 200 2287 \"-\" \"Go-http-client/1.1\"\n",
      "2025-04-04 01:54:50,332 | root | INFO | 200\n",
      "127.0.0.1 - - [04/Apr/2025:01:54:50 +0000] \"GET /swagger.json HTTP/1.0\" 200 2287 \"-\" \"Go-http-client/1.1\"\n",
      "\n",
      "El servicio requiere autenticación.\n",
      "Clave principal: gFuXVMJKwbiprOdU4dMK8FxCBoRdQ4lw\n",
      "Clave de API guardada en 'api_key.json'\n",
      "\n",
      "    Para usar la API:\n",
      "    1. Ejecuta el script 'api.py' para realizar predicciones\n",
      "    2. O utiliza la URI con tu propia aplicación\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Obtener y mostrar los logs para diagnóstico\n",
    "    print(\"\\nLogs del servicio desplegado:\")\n",
    "    logs = service.get_logs()\n",
    "    print(logs)\n",
    "    \n",
    "    # Guardar la URI en un archivo JSON\n",
    "    scoreuri = json.dumps({\"URI\": [scoring_uri]})\n",
    "    with open(\"uri.json\", \"w\") as file:\n",
    "        file.write(scoreuri)\n",
    "    \n",
    "    # Guardar las credenciales de autenticación\n",
    "    if service.auth_enabled:\n",
    "        print(\"El servicio requiere autenticación.\")\n",
    "        print(f\"Clave principal: {service.get_keys()[0]}\")\n",
    "        # Guardar la clave para uso futuro\n",
    "        with open(\"api_key.json\", \"w\") as key_file:\n",
    "            json.dump({\"key\": service.get_keys()[0]}, key_file)\n",
    "            print(\"Clave de API guardada en 'api_key.json'\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Para usar la API:\n",
    "    1. Ejecuta el script 'api.py' para realizar predicciones\n",
    "    2. O utiliza la URI con tu propia aplicación\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al obtener información del servicio: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
